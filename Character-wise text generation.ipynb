{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in december 2019, the outbreak of pneumonia caused by a novel coronavirus, severe acute \n",
      "respiratory syndrome coronavirus 2 (sars-cov-2), has led to a serious pandemic in china and other \n",
      "countries worldwide. so far, more than 460,000 confirmed cases were diagnosed in nearly 190 \n",
      "countries, causing globally over 20,000 deaths. currently, the epidemic is still spreading and there is \n",
      "no effective m\n"
     ]
    }
   ],
   "source": [
    "'I converted .csv file dataset into .txt file'\n",
    "data = open('covid-19_article.txt','r').read().lower()\n",
    "print(data[:400])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 36416 total characters and 56 unique characters in data.\n"
     ]
    }
   ],
   "source": [
    "chars = list(set(data))\n",
    "data_len = len(data)\n",
    "char_len = len(chars)      #vocab size\n",
    "print(\"There are %d total characters and %d unique characters in data.\" %((data_len, char_len)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = sorted(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_to_index = {ch:i for i,ch in enumerate(chars)}\n",
    "int_text = np.array([char_to_index[i] for i in data])\n",
    "index_to_char = np.array(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 150\n",
    "char_dataset = tf.data.Dataset.from_tensor_slices(int_text)\n",
    "sequences = char_dataset.batch(seq_length+1, drop_remainder = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_input_target_pair(chunk):\n",
    "    input_text = chunk[:-1]\n",
    "    target_text = chunk[1:]\n",
    "    return input_text, target_text\n",
    "\n",
    "dataset = sequences.map(create_input_target_pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data:  'in december 2019, the outbreak of pneumonia caused by a novel coronavirus, severe acute \\nrespiratory syndrome coronavirus 2 (sars-cov-2), has led to a'\n",
      "Target data: 'n december 2019, the outbreak of pneumonia caused by a novel coronavirus, severe acute \\nrespiratory syndrome coronavirus 2 (sars-cov-2), has led to a '\n"
     ]
    }
   ],
   "source": [
    "for input_example, target_example in  dataset.take(1):\n",
    "  print ('Input data: ', repr(''.join(index_to_char[input_example.numpy()])))\n",
    "  print ('Target data:', repr(''.join(index_to_char[target_example.numpy()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((64, 150), (64, 150)), types: (tf.int32, tf.int32)>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Batch_size = 64\n",
    "buffer_size = 10000\n",
    "dataset = dataset.shuffle(buffer_size).batch(Batch_size, drop_remainder=True)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 256\n",
    "rnn_units = 1024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## text generator model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_model(char_len , embedding_dim , rnn_units, batch_size):\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Embedding(char_len, embedding_dim, batch_input_shape = [batch_size, None]),\n",
    "        tf.keras.layers.LSTM(rnn_units, return_sequences=True, stateful=True, recurrent_initializer='glorot_uniform'),\n",
    "        tf.keras.layers.Dense(char_len)\n",
    "    ])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = text_model(char_len, embedding_dim, rnn_units, Batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "'Only testing the shape'\n",
    "for input_example_batch, target_example_batch in dataset.take(1):\n",
    "    example_prediction = model(input_example_batch)\n",
    "    assert (example_prediction.shape == (Batch_size, seq_length, char_len)), \"Shape error\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_indices = tf.random.categorical(example_prediction[0], num_samples=1)\n",
    "sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(labels, logits):\n",
    "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction shape:  (64, 150, 56)\n",
      "Loss:       4.0254836\n"
     ]
    }
   ],
   "source": [
    "example_loss  = loss(target_example_batch, example_prediction)\n",
    "print(\"Prediction shape: \", example_prediction.shape)\n",
    "print(\"Loss:      \", example_loss.numpy().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_dir_checkpoints= './training_checkpoints_LSTM'\n",
    "checkpoint_prefix = os.path.join(lstm_dir_checkpoints, \"checkpt_{epoch}\") \n",
    "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_prefix,save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 110"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/110\n",
      "3/3 [==============================] - 13s 3s/step - loss: 3.8071\n",
      "Epoch 2/110\n",
      "3/3 [==============================] - 11s 4s/step - loss: 3.3029\n",
      "Epoch 3/110\n",
      "3/3 [==============================] - 12s 4s/step - loss: 3.1496\n",
      "Epoch 4/110\n",
      "3/3 [==============================] - 11s 4s/step - loss: 3.1084\n",
      "Epoch 5/110\n",
      "3/3 [==============================] - 11s 4s/step - loss: 3.0819\n",
      "Epoch 6/110\n",
      "3/3 [==============================] - 11s 4s/step - loss: 3.0768\n",
      "Epoch 7/110\n",
      "3/3 [==============================] - 12s 4s/step - loss: 3.0561\n",
      "Epoch 8/110\n",
      "3/3 [==============================] - 12s 4s/step - loss: 3.0381\n",
      "Epoch 9/110\n",
      "3/3 [==============================] - 11s 4s/step - loss: 3.0174\n",
      "Epoch 10/110\n",
      "3/3 [==============================] - 12s 4s/step - loss: 2.9946\n",
      "Epoch 11/110\n",
      "3/3 [==============================] - 12s 4s/step - loss: 2.9546\n",
      "Epoch 12/110\n",
      "3/3 [==============================] - 12s 4s/step - loss: 2.9340\n",
      "Epoch 13/110\n",
      "3/3 [==============================] - 12s 4s/step - loss: 2.9033\n",
      "Epoch 14/110\n",
      "3/3 [==============================] - 12s 4s/step - loss: 2.8413\n",
      "Epoch 15/110\n",
      "3/3 [==============================] - 12s 4s/step - loss: 2.7853\n",
      "Epoch 16/110\n",
      "3/3 [==============================] - 13s 4s/step - loss: 2.7246\n",
      "Epoch 17/110\n",
      "3/3 [==============================] - 13s 4s/step - loss: 2.6695\n",
      "Epoch 18/110\n",
      "3/3 [==============================] - 13s 4s/step - loss: 2.6246\n",
      "Epoch 19/110\n",
      "3/3 [==============================] - 12s 4s/step - loss: 2.5803\n",
      "Epoch 20/110\n",
      "3/3 [==============================] - 12s 4s/step - loss: 2.6111\n",
      "Epoch 21/110\n",
      "3/3 [==============================] - 12s 4s/step - loss: 2.5530\n",
      "Epoch 22/110\n",
      "3/3 [==============================] - 12s 4s/step - loss: 2.5339\n",
      "Epoch 23/110\n",
      "3/3 [==============================] - 12s 4s/step - loss: 2.4963\n",
      "Epoch 24/110\n",
      "3/3 [==============================] - 12s 4s/step - loss: 2.5813\n",
      "Epoch 25/110\n",
      "3/3 [==============================] - 12s 4s/step - loss: 2.5282\n",
      "Epoch 26/110\n",
      "3/3 [==============================] - 12s 4s/step - loss: 2.4451\n",
      "Epoch 27/110\n",
      "3/3 [==============================] - 12s 4s/step - loss: 2.4177\n",
      "Epoch 28/110\n",
      "3/3 [==============================] - 12s 4s/step - loss: 2.3912\n",
      "Epoch 29/110\n",
      "3/3 [==============================] - 12s 4s/step - loss: 2.3706\n",
      "Epoch 30/110\n",
      "3/3 [==============================] - 12s 4s/step - loss: 2.3436\n",
      "Epoch 31/110\n",
      "3/3 [==============================] - 12s 4s/step - loss: 2.3165\n",
      "Epoch 32/110\n",
      "3/3 [==============================] - 12s 4s/step - loss: 2.3088\n",
      "Epoch 33/110\n",
      "3/3 [==============================] - 11s 4s/step - loss: 2.2886\n",
      "Epoch 34/110\n",
      "3/3 [==============================] - 12s 4s/step - loss: 2.2664\n",
      "Epoch 35/110\n",
      "3/3 [==============================] - 12s 4s/step - loss: 2.2425\n",
      "Epoch 36/110\n",
      "3/3 [==============================] - 12s 4s/step - loss: 2.2201\n",
      "Epoch 37/110\n",
      "3/3 [==============================] - 12s 4s/step - loss: 2.2029\n",
      "Epoch 38/110\n",
      "3/3 [==============================] - 12s 4s/step - loss: 2.1814\n",
      "Epoch 39/110\n",
      "3/3 [==============================] - 12s 4s/step - loss: 2.1604\n",
      "Epoch 40/110\n",
      "3/3 [==============================] - 12s 4s/step - loss: 2.1458\n",
      "Epoch 41/110\n",
      "3/3 [==============================] - 12s 4s/step - loss: 2.1239\n",
      "Epoch 42/110\n",
      "3/3 [==============================] - 12s 4s/step - loss: 2.1130\n",
      "Epoch 43/110\n",
      "3/3 [==============================] - 11s 4s/step - loss: 2.0780\n",
      "Epoch 44/110\n",
      "3/3 [==============================] - 12s 4s/step - loss: 2.0763\n",
      "Epoch 45/110\n",
      "3/3 [==============================] - 12s 4s/step - loss: 2.0394\n",
      "Epoch 46/110\n",
      "3/3 [==============================] - 12s 4s/step - loss: 2.0150\n",
      "Epoch 47/110\n",
      "3/3 [==============================] - 12s 4s/step - loss: 1.9927\n",
      "Epoch 48/110\n",
      "3/3 [==============================] - 12s 4s/step - loss: 1.9741\n",
      "Epoch 49/110\n",
      "3/3 [==============================] - 12s 4s/step - loss: 1.9585\n",
      "Epoch 50/110\n",
      "3/3 [==============================] - 12s 4s/step - loss: 1.9334\n",
      "Epoch 51/110\n",
      "3/3 [==============================] - 13s 5s/step - loss: 1.8993\n",
      "Epoch 52/110\n",
      "3/3 [==============================] - 12s 4s/step - loss: 1.8930\n",
      "Epoch 53/110\n",
      "3/3 [==============================] - 12s 4s/step - loss: 1.8631\n",
      "Epoch 54/110\n",
      "3/3 [==============================] - 13s 4s/step - loss: 1.8421\n",
      "Epoch 55/110\n",
      "3/3 [==============================] - 12s 4s/step - loss: 1.8186\n",
      "Epoch 56/110\n",
      "3/3 [==============================] - 12s 4s/step - loss: 1.7731\n",
      "Epoch 57/110\n",
      "3/3 [==============================] - 12s 4s/step - loss: 1.7667\n",
      "Epoch 58/110\n",
      "3/3 [==============================] - 12s 4s/step - loss: 1.7520\n",
      "Epoch 59/110\n",
      "3/3 [==============================] - 12s 4s/step - loss: 1.7249\n",
      "Epoch 60/110\n",
      "3/3 [==============================] - 12s 4s/step - loss: 1.6946\n",
      "Epoch 61/110\n",
      "3/3 [==============================] - 12s 4s/step - loss: 1.6711\n",
      "Epoch 62/110\n",
      "3/3 [==============================] - 12s 4s/step - loss: 1.6570\n",
      "Epoch 63/110\n",
      "3/3 [==============================] - 12s 4s/step - loss: 1.6271\n",
      "Epoch 64/110\n",
      "3/3 [==============================] - 12s 4s/step - loss: 1.6030\n",
      "Epoch 65/110\n",
      "3/3 [==============================] - 12s 4s/step - loss: 1.5813\n",
      "Epoch 66/110\n",
      "3/3 [==============================] - 12s 4s/step - loss: 1.5411\n",
      "Epoch 67/110\n",
      "3/3 [==============================] - 12s 4s/step - loss: 1.5309\n",
      "Epoch 68/110\n",
      "3/3 [==============================] - 12s 4s/step - loss: 1.4852\n",
      "Epoch 69/110\n",
      "3/3 [==============================] - 12s 4s/step - loss: 1.4629\n",
      "Epoch 70/110\n",
      "3/3 [==============================] - 12s 4s/step - loss: 1.4306\n",
      "Epoch 71/110\n",
      "3/3 [==============================] - 12s 4s/step - loss: 1.4077\n",
      "Epoch 72/110\n",
      "3/3 [==============================] - 12s 4s/step - loss: 1.3729\n",
      "Epoch 73/110\n",
      "3/3 [==============================] - 12s 4s/step - loss: 1.3587\n",
      "Epoch 74/110\n",
      "3/3 [==============================] - 12s 4s/step - loss: 1.3334\n",
      "Epoch 75/110\n",
      "3/3 [==============================] - 13s 4s/step - loss: 1.3180\n",
      "Epoch 76/110\n",
      "3/3 [==============================] - 12s 4s/step - loss: 1.2997\n",
      "Epoch 77/110\n",
      "3/3 [==============================] - 13s 4s/step - loss: 1.2647\n",
      "Epoch 78/110\n",
      "3/3 [==============================] - 13s 4s/step - loss: 1.2224\n",
      "Epoch 79/110\n",
      "3/3 [==============================] - 13s 5s/step - loss: 1.2241\n",
      "Epoch 80/110\n",
      "3/3 [==============================] - 12s 4s/step - loss: 1.1727\n",
      "Epoch 81/110\n",
      "3/3 [==============================] - 12s 4s/step - loss: 1.1454\n",
      "Epoch 82/110\n",
      "3/3 [==============================] - 13s 4s/step - loss: 1.1110\n",
      "Epoch 83/110\n",
      "3/3 [==============================] - 13s 4s/step - loss: 1.0795\n",
      "Epoch 84/110\n",
      "3/3 [==============================] - 13s 4s/step - loss: 1.0511\n",
      "Epoch 85/110\n",
      "3/3 [==============================] - 13s 4s/step - loss: 1.0338\n",
      "Epoch 86/110\n",
      "3/3 [==============================] - 13s 4s/step - loss: 0.9902\n",
      "Epoch 87/110\n",
      "3/3 [==============================] - 13s 4s/step - loss: 0.9752\n",
      "Epoch 88/110\n",
      "3/3 [==============================] - 12s 4s/step - loss: 0.9393\n",
      "Epoch 89/110\n",
      "3/3 [==============================] - 13s 5s/step - loss: 0.9222\n",
      "Epoch 90/110\n",
      "3/3 [==============================] - 12s 4s/step - loss: 0.8898\n",
      "Epoch 91/110\n",
      "3/3 [==============================] - 12s 4s/step - loss: 0.8600\n",
      "Epoch 92/110\n",
      "3/3 [==============================] - 13s 4s/step - loss: 0.8246\n",
      "Epoch 93/110\n",
      "3/3 [==============================] - 12s 4s/step - loss: 0.7890\n",
      "Epoch 94/110\n",
      "3/3 [==============================] - 13s 4s/step - loss: 0.7530\n",
      "Epoch 95/110\n",
      "3/3 [==============================] - 13s 4s/step - loss: 0.7328\n",
      "Epoch 96/110\n",
      "3/3 [==============================] - 13s 4s/step - loss: 0.6944\n",
      "Epoch 97/110\n",
      "3/3 [==============================] - 14s 5s/step - loss: 0.6662\n",
      "Epoch 98/110\n",
      "3/3 [==============================] - 13s 4s/step - loss: 0.6413\n",
      "Epoch 99/110\n",
      "3/3 [==============================] - 13s 4s/step - loss: 0.6170\n",
      "Epoch 100/110\n",
      "3/3 [==============================] - 13s 4s/step - loss: 0.5892\n",
      "Epoch 101/110\n",
      "3/3 [==============================] - 13s 4s/step - loss: 0.5663\n",
      "Epoch 102/110\n",
      "3/3 [==============================] - 12s 4s/step - loss: 0.6023\n",
      "Epoch 103/110\n",
      "3/3 [==============================] - 12s 4s/step - loss: 0.5616\n",
      "Epoch 104/110\n",
      "3/3 [==============================] - 13s 4s/step - loss: 0.5230\n",
      "Epoch 105/110\n",
      "3/3 [==============================] - 13s 4s/step - loss: 0.4930\n",
      "Epoch 106/110\n",
      "3/3 [==============================] - 12s 4s/step - loss: 0.4674\n",
      "Epoch 107/110\n",
      "3/3 [==============================] - 12s 4s/step - loss: 0.4446\n",
      "Epoch 108/110\n",
      "3/3 [==============================] - 13s 4s/step - loss: 0.4185\n",
      "Epoch 109/110\n",
      "3/3 [==============================] - 13s 4s/step - loss: 0.4043\n",
      "Epoch 110/110\n",
      "3/3 [==============================] - 13s 4s/step - loss: 0.3805\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(dataset, epochs=epochs, callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./training_checkpoints_LSTM\\\\checkpt_110'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.train.latest_checkpoint(lstm_dir_checkpoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (1, None, 256)            14336     \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (1, None, 1024)           5246976   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (1, None, 56)             57400     \n",
      "=================================================================\n",
      "Total params: 5,318,712\n",
      "Trainable params: 5,318,712\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = text_model(char_len, embedding_dim, rnn_units, batch_size=1)\n",
    "model.load_weights(tf.train.latest_checkpoint(lstm_dir_checkpoints))\n",
    "model.build(tf.TensorShape([1, None]))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, start_string):\n",
    "    num_generate = 1000 \n",
    "\n",
    "    input_eval = [char_to_index[s] for s in start_string] \n",
    "    input_eval = tf.expand_dims(input_eval, 0)\n",
    "\n",
    "    text_generated = []\n",
    "\n",
    "    \n",
    "    temperature = 0.5\n",
    "\n",
    "    \n",
    "    model.reset_states()\n",
    "    for i in range(num_generate):\n",
    "        predictions = model(input_eval)\n",
    "        \n",
    "        predictions = tf.squeeze(predictions, 0)\n",
    "\n",
    "        \n",
    "        predictions = predictions / temperature\n",
    "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
    "\n",
    "        \n",
    "        input_eval = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "        text_generated.append(index_to_char[predicted_id])\n",
    "\n",
    "    return (start_string + ''.join(text_generated))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your starting string: severe acute respiratory syndrome-coronavirus-2 outbreak has rapidly reached pandemic proportions and has become a major threat to global health\n",
      "\n",
      "severe acute respiratory syndrome-coronavirus-2 outbreak has rapidly reached pandemic proportions and has become a major threat to global health. the \n",
      "peastices of the ever coronavirus (sars-cov) and migdlies and cintrean parameters. healthcare profession and ment mananement vaccine are \n",
      "proved with and martorment, and 55% co from tha specific to be of the covid-19 epidemic. this is an infection cases. melicully human to have of eninome care as in the transmission this network has and engeriance treatment options war computien werious and infectinule treamment of patients wite sederialogical and nampreas. as for offective wat 13%, the world ant hovical encection and \n",
      "sectormations is the stodnts in covid-19 patients, with \n",
      "covid-19 coult im/hevent and proved to infection prevention and control of the disease canament of severe acute respiratory syndrome coronavirus 2 (sars-cov-2) is becoming available, but a synthesis of available data has not been conducted. we propose a multiple-information susce seceine mata on marca clinical transmission resputs, a late rate and control matagities in the effiction of continme the number of\n"
     ]
    }
   ],
   "source": [
    "test = input(\"Enter your starting string: \")\n",
    "print()\n",
    "print(generate_text(model, start_string=test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
