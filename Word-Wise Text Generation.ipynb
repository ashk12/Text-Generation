{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word-Wise Text Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import string\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Input, Dropout, LSTM, Activation\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.initializers import glorot_uniform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = open('covid-19_article.txt','r').read().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 5310 total tokens and 1618 unique tokens in data\n"
     ]
    }
   ],
   "source": [
    "tokens = data.split()\n",
    "'remove punctuations form string'\n",
    "table = str.maketrans('', '', string.punctuation)\n",
    "tokens = [w.translate(table) for w in tokens]\n",
    "print(\"There are %d total tokens and %d unique tokens in data\" %((len(tokens),len(set(tokens)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Sequences: 5259\n"
     ]
    }
   ],
   "source": [
    "'Making sequences of length 50'\n",
    "length = 50 + 1\n",
    "sequences = list()\n",
    "for i in range(length, len(tokens)):\n",
    "    seq = tokens[i-length:i]\n",
    "    line = ' '.join(seq)\n",
    "    sequences.append(line)\n",
    "print('Total Sequences: %d' % len(sequences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating new text file in which each lines are of length 50  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_doc(lines, filename):\n",
    "    data = '\\n'.join(lines)\n",
    "    file = open(filename, 'w')\n",
    "    file.write(data)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_filename = 'covid_data.txt'\n",
    "save_doc(sequences, out_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq = open('covid_data.txt','r').read()\n",
    "lines = seq.split('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer function converts each lines of words into numbers with the help of LabelEncoder() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Tokenizer(lines):\n",
    "    tokenizer = LabelEncoder()\n",
    "    seq_gen = tokenizer.fit(tokens)\n",
    "    sequences = []\n",
    "    for i in range(len(lines)):\n",
    "        temp = lines[i].split(' ')\n",
    "        sequences.append(seq_gen.transform(temp))\n",
    "    return sequences,seq_gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences,tokenizer = Tokenizer(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1618"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = len(tokenizer.classes_)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating dataset for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5259, 50)\n",
      "(5259, 1618)\n"
     ]
    }
   ],
   "source": [
    "sequences = np.array(sequences)\n",
    "X, y = sequences[:,:-1], sequences[:,-1]\n",
    "print(X.shape)\n",
    "y = np.eye(vocab_size)[y.reshape(-1)]\n",
    "print(y.shape)\n",
    "seq_length = X.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Glove vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = dict()\n",
    "f = open('glove.6B.50d.txt','r',encoding = 'utf8')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "print('Loaded %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((vocab_size, 50))\n",
    "token_obj = dict(zip(tokenizer.classes_, tokenizer.transform(tokenizer.classes_)))\n",
    "for word, i in token_obj.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_generator(Input_shape):\n",
    "    \n",
    "    sentence_indices = Input(Input_shape)\n",
    "    embedding_layer = Embedding(vocab_size, 50, trainable=False)\n",
    "    embedding_layer.build((None,))\n",
    "    embedding_layer.set_weights([embedding_matrix])\n",
    "    embeddings = embedding_layer(sentence_indices)\n",
    "    \n",
    "    X = LSTM(400 , return_sequences = True)(embeddings)\n",
    "    X = LSTM(400 , return_sequences = False)(X)\n",
    "    X = Dense(128 , activation = 'relu')(X)\n",
    "    X = Dense(vocab_size , activation = 'softmax')(X)\n",
    "    \n",
    "    model = Model(sentence_indices , X)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = text_generator((seq_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss = 'categorical_crossentropy' , optimizer = 'adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "165/165 [==============================] - 187s 893ms/step - loss: 6.8258\n",
      "Epoch 2/50\n",
      "165/165 [==============================] - 145s 876ms/step - loss: 6.2691\n",
      "Epoch 3/50\n",
      "165/165 [==============================] - 144s 875ms/step - loss: 6.2280\n",
      "Epoch 4/50\n",
      "165/165 [==============================] - 147s 889ms/step - loss: 6.2416\n",
      "Epoch 5/50\n",
      "165/165 [==============================] - 146s 883ms/step - loss: 6.2159\n",
      "Epoch 6/50\n",
      "165/165 [==============================] - 92s 558ms/step - loss: 6.0800\n",
      "Epoch 7/50\n",
      "165/165 [==============================] - 90s 547ms/step - loss: 5.9801\n",
      "Epoch 8/50\n",
      "165/165 [==============================] - 92s 558ms/step - loss: 5.8406\n",
      "Epoch 9/50\n",
      "165/165 [==============================] - 91s 551ms/step - loss: 5.6863\n",
      "Epoch 10/50\n",
      "165/165 [==============================] - 91s 551ms/step - loss: 5.4847\n",
      "Epoch 11/50\n",
      "165/165 [==============================] - 91s 553ms/step - loss: 5.3147\n",
      "Epoch 12/50\n",
      "165/165 [==============================] - 90s 548ms/step - loss: 5.0042\n",
      "Epoch 13/50\n",
      "165/165 [==============================] - 91s 550ms/step - loss: 4.6856\n",
      "Epoch 14/50\n",
      "165/165 [==============================] - 92s 558ms/step - loss: 4.4398\n",
      "Epoch 15/50\n",
      "165/165 [==============================] - 86s 521ms/step - loss: 4.1936\n",
      "Epoch 16/50\n",
      "165/165 [==============================] - 85s 516ms/step - loss: 3.8644\n",
      "Epoch 17/50\n",
      "165/165 [==============================] - 86s 524ms/step - loss: 3.5720\n",
      "Epoch 18/50\n",
      "165/165 [==============================] - 86s 521ms/step - loss: 3.2744\n",
      "Epoch 19/50\n",
      "165/165 [==============================] - 86s 522ms/step - loss: 2.9114\n",
      "Epoch 20/50\n",
      "165/165 [==============================] - 86s 521ms/step - loss: 2.5862\n",
      "Epoch 21/50\n",
      "165/165 [==============================] - 86s 522ms/step - loss: 2.1585\n",
      "Epoch 22/50\n",
      "165/165 [==============================] - 87s 528ms/step - loss: 1.8591\n",
      "Epoch 23/50\n",
      "165/165 [==============================] - 85s 518ms/step - loss: 1.5158\n",
      "Epoch 24/50\n",
      "165/165 [==============================] - 87s 530ms/step - loss: 1.2068\n",
      "Epoch 25/50\n",
      "165/165 [==============================] - 89s 536ms/step - loss: 0.9479\n",
      "Epoch 26/50\n",
      "165/165 [==============================] - 88s 533ms/step - loss: 0.7087\n",
      "Epoch 27/50\n",
      "165/165 [==============================] - 85s 517ms/step - loss: 0.5106\n",
      "Epoch 28/50\n",
      "165/165 [==============================] - 85s 515ms/step - loss: 0.3874\n",
      "Epoch 29/50\n",
      "165/165 [==============================] - 86s 520ms/step - loss: 0.2643\n",
      "Epoch 30/50\n",
      "160/165 [============================>.] - ETA: 2s - loss: 0.1747"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-f8126b2c2fef>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m50\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m32\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1156\u001b[0m                 _r=1):\n\u001b[0;32m   1157\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1158\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1159\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1160\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    887\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    891\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    915\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    916\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 917\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    918\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    919\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3022\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m   3023\u001b[0m     return graph_function._call_flat(\n\u001b[1;32m-> 3024\u001b[1;33m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[0;32m   3025\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3026\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1959\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1960\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1961\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1962\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1963\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    594\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    595\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 596\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    597\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    598\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[1;32m---> 60\u001b[1;33m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "'I have only trained till 25 because it takes long time to train the model'\n",
    "model.fit(X , y , epochs = 50 , batch_size = 32 , shuffle = True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 50)]              0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (None, 50, 50)            80900     \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 50, 400)           721600    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 400)               1281600   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               51328     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1618)              208722    \n",
      "=================================================================\n",
      "Total params: 2,344,150\n",
      "Trainable params: 2,263,250\n",
      "Non-trainable params: 80,900\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text():\n",
    "    res = X[99]\n",
    "    string_mapped = res\n",
    "    string_mapped = list(string_mapped)\n",
    "    for i in range(500):\n",
    "        x = np.reshape(res,(1,seq_length))\n",
    "        y_pred = model.predict(x,verbose=0)\n",
    "        pred = np.argmax(y_pred)\n",
    "        string_mapped.append(pred)\n",
    "        res = list(res)\n",
    "        res = res[1:]\n",
    "        res.append(pred)\n",
    "        res = np.array(res)\n",
    "    print('\\n\\nOutput:')\n",
    "    print('\\'',' '.join(tokenizer.inverse_transform(string_mapped)),'\\'')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Output:\n",
      "' most of the vaccines are under design and preparation there are some that have entered efficacy evaluation in animals and initial clinical trials this review mainly focused on the progress and our prospects on field of vaccine development against sarscov2 18 years ago in 2002 the world was astonished by the appearance of severe acute respiratory syndrome sars supported by a zoonotic coronavirus called sarscov from the guangdong province of southern china after about 10 years in 2012 another similar coronavirus triggered the middle east respiratory syndrome merscov in saudi arabia both caused severe pneumonia killing 774 and 858 people with 8700 cases of confirmed infection for the former and 2494 for the latter causing significant economic losses 8 years later despite the mers outbreak remaining in certain parts of the world at the end of 2019 a new zoonotic coronavirus sarscov2 and responsible of coronavirus disease covid19 arose from wuhan hubei province china it spread rapidly and to date has killed 3242 persons with more than 81000 cases of infection in china and there different important in the pandemic dynamics and remain programs and infection is month in infected cases by sarscov2 cases in wuhan province in china is rapidly spreading pheic the middle east the viral dynamics recommendations as significantly we substantial the patterns of specific data and help a most time express data including deforestation control control infodemic and other no effective background and infection is month by covid19 and an bat role in sarscov2 cases with significantly from hubei province in central china to neighbouring areas logistic model was employed according to the trend of available data which shows the difference between hubei province and outside of it we also calculated the reproduction number r0 for the range of 223 251 via seir model the measures to reduce or prevent the virus spread should be implemented and we expect our data driven modeling analysis providing some insights to identify and prepare for the future virus control the outbreak of covid19 caused by sarscov2 in wuhan and other cities of china is a growing global concern delay in diagnosis and limited hospital resources lead to a rapid spread of covid19 in this study we investigate the effect of delay in diagnosis on the disease transmission with a new formulated dynamic model sensitivity analyses and numerical simulations reveal that improving the proportion of timely diagnosis and shortening the waiting time for diagnosis can not eliminate covid19 but can effectively decrease the basic reproduction number significantly reduce the transmission risk and effectively prevent the endemic of covid19 eg shorten the peak time and reduce the peak value of new confirmed cases and new infection decrease the cumulative number of confirmed cases and total infection more rigorous prevention measures and better treatment of patients are needed to control its further spread eg increasing available hospital beds shortening the period from symptom onset to isolation of patients quarantining and isolating the suspected cases as well as all confirmed patients background the psychological and behavioral responses during the early stage of coronavirus disease 2019 covid19 in south korea were investigated to guide the public as full and active participants of public health emergency preparedness phep which is essential to improving resilience and reducing the populations fundamental vulnerability methods data were collected through '\n"
     ]
    }
   ],
   "source": [
    "generate_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
